{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reference :\n",
    "\n",
    "- https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a\n",
    "- https://www.youtube.com/watch?v=MnRskV3NY1k&t=1571s&ab_channel=T%C3%BCbingenMachineLearning\n",
    "- https://www.dailydoseofds.com/formulating-and-implementing-the-t-sne-algorithm-from-scratch/"
   ],
   "id": "3060d922da638113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. (5 pts) Dimension Reduction Using t-SNE\n",
    "\n",
    "## 2.1 (2 pts) Please build the t-SNE algorithm from scratch based on the equations below:\n",
    "\n",
    "Please cite if you are referring to any source for the algorithms.\n",
    "There are many hyperparameters to optimize, such as initialization (random seed), learning rate ùúÜ , momentum\n",
    "ùõº(ùë°), iteration number, and perplexity.\n",
    "\n"
   ],
   "id": "ef19bd2f1ca0b25f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\n",
    "P_{j|i} = \\frac{\\exp\\left(- \\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(- \\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)}\n",
    "\n",
    "\\\\ \n",
    "\n",
    "\\[\n",
    "q_{j|i} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
    "\\]\n",
    "\n",
    "\\\\ \n",
    "\n",
    "\\[\n",
    "C = KL(P||Q) = \\sum_i \\sum_j P_{j|i} \\log \\frac{P_{j|i}}{q_{j|i}}\n",
    "\\]\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}\n",
    "\\]\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\[\n",
    "y^{(t)} = y^{(t-1)} + \\lambda \\frac{\\partial C}{\\partial y} + \\alpha(t)(y^{(t-1)} - y^{(t-2)})\n",
    "\\]\n",
    "\n",
    "$$"
   ],
   "id": "b31cd7a50379b5bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:40:42.149666Z",
     "start_time": "2024-10-21T06:40:41.840057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fashion_mnist1.csv')\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "print(f\"{X.shape = }, {y.shape = }\")"
   ],
   "id": "4ef3e59ddffca263",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (10000, 784), y.shape = (10000,)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-21T06:40:54.887489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class TSNE:\n",
    "    \n",
    "    def __init__(self, perplexity=10, iterations=1000, early_exaggeration=4,n_components=2, learning_rate=100, momentum=0.5, initialization_method ='random', random_state=42):\n",
    "        \"\"\"\n",
    "        t-SNE algorithm parameters\n",
    "            :param perplexity: Perplexity\n",
    "            :param iterations: number of Iterations the gradient descent algorithm will run\n",
    "            :param early_exaggeration: Early Exaggeration\n",
    "            :param n_components: Number of components\n",
    "            :param learning_rate: Learning Rate\n",
    "            :param momentum: momentum\n",
    "            :param initialization_method: Initialization method for the t-SNE\n",
    "            :param random_state: Random State\n",
    "        \"\"\"\n",
    "        self.perplexity = perplexity\n",
    "        self.iterations = iterations\n",
    "        self.early_exaggeration = early_exaggeration\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.initialization_method = initialization_method\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def _find_sigma(self, norm, i, perplexity):\n",
    "        \"\"\"\n",
    "        Helper function to obtain œÉ's based on user-specified perplexity.\n",
    "            :param norm: pairwise squared differences between data points\n",
    "            :param i: Iteration number\n",
    "            :param perplexity: desired Perplexity\n",
    "        \n",
    "        return: sigma that satisfies the perplexity condition.\n",
    "        \"\"\"\n",
    "        result = np.inf  # Set first result to be infinity\n",
    "        std_norm = np.std(norm)  # Use standard deviation of norms to define search space\n",
    "        sigma = 0\n",
    "        for sigma_i in np.linspace(0.01 * std_norm, 5 * std_norm, 200):\n",
    "            # Equation 1 Numerator\n",
    "            p = np.exp(-(norm**2) / (2 * sigma_i**2))\n",
    "            p[i] = 0\n",
    "    \n",
    "            p_new = np.maximum(p / np.sum(p), sys.float_info.min)\n",
    "    \n",
    "            # Shannon Entropy\n",
    "            H = -np.sum(p_new * np.log2(p_new))\n",
    "    \n",
    "            # Get log(perplexity equation) as close to equality\n",
    "            if np.abs(np.log(perplexity) - H * np.log(2)) < np.abs(result):\n",
    "                result = np.log(perplexity) - H * np.log(2)\n",
    "                sigma = sigma_i\n",
    "    \n",
    "        return sigma\n",
    "    \n",
    "    def get_original_pairwise_affinities(self, X, perplexity):\n",
    "        \"\"\"\n",
    "        param:\n",
    "            X - input data\n",
    "            perplexity: perplexity\n",
    "    \n",
    "        return: original high dimension probability\n",
    "        \"\"\"\n",
    "    \n",
    "        n = X.shape[0]\n",
    "        print(\"Starting the high dimension computation...\")\n",
    "        p_ij = np.zeros(shape=(n, n))\n",
    "        with tqdm(total=n) as _:\n",
    "            for i in range(0, n):\n",
    "                diff = X[i] - X\n",
    "                \n",
    "                norm = np.linalg.norm(diff, axis=1)\n",
    "                sigma_i = self._find_sigma(norm, i, perplexity)  \n",
    "                p_ij[i, :] = np.exp(-(norm**2) / (2 * sigma_i**2))\n",
    "                np.fill_diagonal(p_ij, 0)\n",
    "                p_ij[i, :] = p_ij[i, :] / np.sum(p_ij[i, :])\n",
    "    \n",
    "        p_ij = np.maximum(p_ij, sys.float_info.min)\n",
    "        print(\"Completed the high dimension computation...\")\n",
    "        return p_ij\n",
    "    \n",
    "    def get_symmetric_p_ij(self, p_ij):\n",
    "        \"\"\"\n",
    "        Function to obtain symmetric affinities matrix utilized in t-SNE.\n",
    "    \n",
    "        Parameters:\n",
    "        p_ij (np.ndarray): The input affinity matrix.\n",
    "    \n",
    "        Returns:\n",
    "        np.ndarray: The symmetric affinities matrix.\n",
    "    \n",
    "        \"\"\"\n",
    "        # print(\"Computing Symmetric p_ij matrix....\")\n",
    "        # \n",
    "        # n = len(p_ij)\n",
    "        # p_ij_symmetric = np.zeros(shape=(n, n))\n",
    "        # for i in range(0, n):\n",
    "        #     for j in range(0, n):\n",
    "        #         p_ij_symmetric[i, j] = (p_ij[i, j] + p_ij[j, i]) / (2 * n)\n",
    "        # \n",
    "        # # Set 0 values to minimum numpy value (Œµ approx. = 0)\n",
    "        # Œµ = np.nextafter(0, 1)\n",
    "        # p_ij_symmetric = np.maximum(p_ij_symmetric, Œµ)\n",
    "        # \n",
    "        # print(\"Completed Symmetric p_ij Matrix. \\n\")\n",
    "        # \n",
    "        # return p_ij_symmetric\n",
    "        n = p_ij.shape[0]\n",
    "        p_ij = (p_ij + p_ij.T) / (2 * n)\n",
    "        p_ij = np.maximum(p_ij, sys.float_info.min)\n",
    "        return p_ij\n",
    "    \n",
    "    def get_low_dimensional_affinities(self, Y):\n",
    "        \"\"\"\n",
    "        Obtain low-dimensional affinities.\n",
    "    \n",
    "        Parameters:\n",
    "        Y (np.ndarray): The low-dimensional representation of the data points.\n",
    "    \n",
    "        Returns:\n",
    "        np.ndarray: The low-dimensional affinities matrix.\n",
    "        \"\"\"\n",
    "    \n",
    "        n = len(Y)\n",
    "        q_ij = np.zeros(shape=(n, n))\n",
    "    \n",
    "        for i in range(0, n):\n",
    "            # Equation 4 Numerator\n",
    "            diff = Y[i] - Y\n",
    "            norm = np.linalg.norm(diff, axis=1)\n",
    "            q_ij[i, :] = (1 + norm**2) ** (-1)\n",
    "    \n",
    "        # Set p = 0 when j = i\n",
    "        np.fill_diagonal(q_ij, 0)\n",
    "    \n",
    "        # Equation 4\n",
    "        q_ij = q_ij / q_ij.sum()\n",
    "    \n",
    "        # Set 0 values to minimum numpy value (Œµ approx. = 0)\n",
    "        q_ij = np.maximum(q_ij, sys.float_info.min)\n",
    "    \n",
    "        return q_ij\n",
    "    \n",
    "    def _gradient(self, p_ij, q_ij, Y):\n",
    "        \"\"\"\n",
    "        Obtain gradient of cost function at current point Y.\n",
    "    \n",
    "        Parameters:\n",
    "        p_ij (np.ndarray): The joint probability distribution matrix.\n",
    "        q_ij (np.ndarray): The Student's t-distribution matrix.\n",
    "        Y (np.ndarray): The current point in the low-dimensional space.\n",
    "    \n",
    "        Returns:\n",
    "        np.ndarray: The gradient of the cost function at the current point Y.\n",
    "        \"\"\"\n",
    "    \n",
    "        n = len(p_ij)\n",
    "    \n",
    "        # Compute gradient\n",
    "        gradient = np.zeros(shape=(n, Y.shape[1]))\n",
    "        for i in range(0, n):\n",
    "            # Equation 5\n",
    "            diff = Y[i] - Y\n",
    "            A = np.array([(p_ij[i, :] - q_ij[i, :])])\n",
    "            B = np.array([(1 + np.linalg.norm(diff, axis=1)) ** (-1)])\n",
    "            C = diff\n",
    "            gradient[i] = 4 * np.sum((A * B).T * C, axis=0)\n",
    "    \n",
    "        return gradient\n",
    "    \n",
    "    def initialization(self, X, initialization_method) :\n",
    "        \"\"\"\n",
    "        Initial t-SNE either randomly or using PCA.\n",
    "            :param X: input data\n",
    "            :param initialization_method: initialization method can be random or PCA\n",
    "        \n",
    "        :return - initial solution for t-SNE.\n",
    "        \"\"\"\n",
    "\n",
    "        if initialization_method == \"PCA\":\n",
    "            print(\"Initializing with PCA...\")\n",
    "            X_centered = X - X.mean(axis=0)\n",
    "            U, S, V = np.linalg.svd(X_centered)\n",
    "            return X_centered @ V.T[:, :self.n_components]\n",
    "        else:\n",
    "            print(f\"Initialization method {initialization_method}, is not PCA, so random initialization is used.\")\n",
    "            return np.random.normal(loc=0, scale=1e-4, size=(X.shape[0], self.n_components))\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        train t-sne for the input data.\n",
    "            :param X: Training data\n",
    "        returns: list of low-dimensional embeddings and the history of embeddings at each iteration.\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "    \n",
    "        # Get original affinities matrix\n",
    "        p_ij = self.get_original_pairwise_affinities(X, self.perplexity)\n",
    "        p_ij = self.get_symmetric_p_ij(p_ij)\n",
    "    \n",
    "        # Initialization\n",
    "        Y = np.zeros(shape=(self.iterations, n, self.n_components))\n",
    "        Y[0] = np.zeros(shape=(n, self.n_components))\n",
    "        Y[1] = np.array(self.initialization(X, self.initialization_method))\n",
    "        costs, distances = [], []\n",
    "    \n",
    "        print(\"Optimizing Low Dimensional Embedding....\")\n",
    "        for t in range(1, self.iterations - 1):\n",
    "                \n",
    "            alpha, early_exaggeration = (0.5, self.early_exaggeration) if t < 250 else (0.8, 1)\n",
    "    \n",
    "            # Get Low Dimensional Affinities\n",
    "            q_ij = self.get_low_dimensional_affinities(Y[t])\n",
    "    \n",
    "            # Get Gradient of Cost Function\n",
    "            gradient = self._gradient(early_exaggeration * p_ij, q_ij, Y[t])\n",
    "            Y[t + 1] = Y[t] - self.learning_rate * gradient + alpha * (Y[t] - Y[t - 1])  \n",
    "    \n",
    "            cost = np.sum(p_ij * np.log(p_ij / q_ij))\n",
    "            costs.append(cost)    \n",
    "            # Compute current value of cost function\n",
    "            if t % 50 == 0 or t == 1:\n",
    "                print(f\"Iteration {t}: Value of Cost Function is {cost}\")\n",
    "    \n",
    "        print(f\"Completed Low Dimensional Embedding: Final Value of Cost Function is {costs[-1]}\")\n",
    "        embedding = Y[-1]\n",
    "        return embedding, Y\n",
    "    \n",
    "    def plot_embedding(self, embedding, y):\n",
    "        fig, ax = plt.subplots()\n",
    "        g1 = ax.scatter(embedding[:, 0], embedding[:, 1], c=y, cmap=\"tab10\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"MNIST t-SNE\")\n",
    "        plt.colorbar(g1, ax=ax)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "n_sample = 1000\n",
    "X_sample = X[:n_sample:, ]\n",
    "y_sample = y[:n_sample]\n",
    "tsne = TSNE(perplexity=10, iterations=500, learning_rate=200, early_exaggeration=4, n_components=2)\n",
    "solution, Y = tsne.fit(X_sample)\n",
    "tsne.plot_embedding(solution, y_sample)\n",
    "\n",
    "# solution, Y = tsne(X_sample, perplexity=10, T=1000, Œ∑=200, early_exaggeration=4, n_dimensions=2)\n"
   ],
   "id": "c9337eb7826472ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the high dimension computation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the high dimension computation...\n",
      "Initialization method random, is not PCA, so random initialization is used.\n",
      "Optimizing Low Dimensional Embedding....\n",
      "Iteration 1: Value of Cost Function is 4.323102073012128\n",
      "Iteration 50: Value of Cost Function is 2.3951779995637823\n",
      "Iteration 100: Value of Cost Function is 2.1327368993575972\n",
      "Iteration 150: Value of Cost Function is 1.9771285550660131\n",
      "Iteration 200: Value of Cost Function is 1.9470797949593066\n",
      "Iteration 250: Value of Cost Function is 1.9372367808155384\n",
      "Iteration 300: Value of Cost Function is 1.1287319527167154\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f62b3659fc7ffb88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.2(1.5pts) Using the t-SNE method, please reduce the 784 dimensions to 2 dimensions. Please try at least 5 different hyperparameters conditions.\n",
    "\n",
    "- For each hyperparameter condition, please calculate its corresponding D and J.\n",
    "- Please calculate the sum of the distance D among the 10 centroids. \n",
    "- Each centroid corresponds to each label. Since there are 10 centroids, you should calculate distance for 45 pairs. \n",
    "- Also, please calculate the objective function J.\n",
    "- Your goal is to maximize the distance D among the centroids and to minimize the objective function J by optimizing the hyperparameters."
   ],
   "id": "5a41ac20102e2245"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\n",
    "D = \\sum_{i,j} \\|y_i - y_j\\|^2\n",
    "\n",
    "\\\\\n",
    "\n",
    "J = \\sum_{i=1}^{k} \\sum_{j=1}^{n} \\|x_i - c_j\\|^2\n",
    "\n",
    "$$"
   ],
   "id": "d4eb9d5b3d145815"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.3(1.5pts)\n",
    "\n",
    "- Please draw 2D plot for the 5 hyperparameter conditions.\n",
    "- Please legend different colors for the 10 labels in the graph."
   ],
   "id": "959f7a645290ceea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:40:51.260758Z",
     "start_time": "2024-10-21T06:40:51.258196Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "80751ce36cd6b301",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:40:49.389985Z",
     "start_time": "2024-10-21T06:40:49.387561Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3f54ae61409760fa",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:05:06.029058Z",
     "start_time": "2024-10-21T06:05:06.021144Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c56eff56f92bb004",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:05:06.034443Z",
     "start_time": "2024-10-21T06:05:06.031001Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6b486e017030acdf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:05:06.042552Z",
     "start_time": "2024-10-21T06:05:06.039114Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c081821bf655a8a",
   "outputs": [],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
